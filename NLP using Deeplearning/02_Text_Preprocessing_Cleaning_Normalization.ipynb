{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 정제(Cleaning) and 정규화(Normalization)\n",
        "## 코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)하는 일이 항상 함께합니다. 정제 및 정규화의 목적은 각각 다음과 같습니다.\n",
        "## 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.\n",
        "## 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."
      ],
      "metadata": {
        "id": "8zkzM1o0LyEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 정제의 방법\n",
        "## 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
        "### 필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로서 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하 는 방법을 사용할 수 있습니다.가령, USA와 US는 같은 의미를 가지므로 하나의 단어로 정규화해볼 수 있습니다. uh-huh와 uhhuh는 형태는 다르지만 여전히 같은 의미를 갖고 있습니다. 이러한 정규화를 거치게 되면, US를 찾아도 USA도 함께 찾을 수 있을 것입니다. 표기가 다른 단어들을 통합하는 방법에는 어간 추출(stemming)과 표제어 추출(lemmatizaiton)이 존재합니다.\n",
        "## 2. 대, 소문자 통합\n",
        "## 3. 불필요한 단어의 제거\n",
        "### 정제 작업에서 제거해야하는 노이즈 데이터(noise data)는 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자 등)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요 단어들을 노이즈 데이터라고 하기도 합니다. 불필요 단어들을 제거하는 방법으로는 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법이 있습니다.\n",
        "## 4. 정규 표현식(Regular Expression)\n",
        "### 얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해서 이를 제거할 수 있는 경우가 많습니다. 가령, HTML 문서로부터 가져온 코퍼스라면 문서 여기저기에 HTML 태그가 있습니다. 뉴스 기사를 크롤링 했다면, 기사마다 게재 시간이 적혀져 있을 수 있습니다. 정규 표현식은 이러한 코퍼스 내에 계속해서 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 방식으로서 매우 유용합니다."
      ],
      "metadata": {
        "id": "3KEfVSEmL_RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A0C4VIkzNim-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 규칙에 기반한 표기가 다른 단어들의 통합 - 표제어 추출 (Lemmatization)\n",
        "## 표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미를 갖습니다. 표제어 추출은 단어들로부터 표제어를 찾아가는 과정입니다. 표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어서 am, are, is는 서로 다른 스펠링이지만 그 뿌리 단어는 be라고 볼 수 있습니다. 이때, 이 단어들의 표제어는 be라고 합니다.\n",
        "## 표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것입니다. 형태소란 '의미를 가진 가장 작은 단위'를 뜻합니다. 그리고 형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문을 뜻합니다. 형태소의 종류로 어간(stem)과 접사(affix)가 존재합니다.\n",
        "### 어간(stem): 단어의 의미를 담고 있는 단어의 핵심 부분\n",
        "### 접사(affix): 단어에 추가적인 의미를 주는 부분\n",
        "## 형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 말합니다."
      ],
      "metadata": {
        "id": "aQqIojVyNisX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 규칙에 기반한 표기가 다른 단어들의 통합 - 어간 추출(Stemming)\n",
        "# NLTK에서 표제어 추출을 위한 도구인 WordNetLemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "\n",
        "print('표제어 추출 전 :',words)\n",
        "print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "'''\n",
        "표제어 추출은 뒤에서 언급할 어간 추출과는 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있습니다.\n",
        "하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있습니다.\n",
        "이는 표제어 추출기(lemmatizer)가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문입니다.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9s0iDqrLyJ1",
        "outputId": "cc3f78c2-cbff-40ba-d032-548355d6f92d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있습니다.\n",
        "# 즉, dies와 watched, has가 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 Lemma를 출력하게 됩니다.\n",
        "\n",
        "lemmatizer.lemmatize('dies', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hhXojr3IOWYM",
        "outputId": "fa6d7288-cb16-4453-f942-572983047b24"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'die'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('watched', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o57ercxuOr1e",
        "outputId": "561bf083-422d-4768-c123-809d5fa6b7f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'watch'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('has', 'v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eFwnDsE4Or31",
        "outputId": "97696f11-f5af-4372-ecc3-add6b7f9fe44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 규칙에 기반한 표기가 다른 단어들의 통합 - 어간 추출 (Stemming)\n",
        "## 어간(Stem)을 추출하는 작업을 어간 추출(stemming)이라고 합니다. 어간 추출은 형태학적 분석을 단순화한 버전이라고 볼 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고 볼 수도 있습니다. 이 작업은 섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다.\n",
        "## 한국어 어간 추출\n",
        "### 한국어는 5언 9품사의 구조를 가지고 있습니다.\n",
        "### 체언: 명사, 대명사, 수사 / 수식언: 관형사, 부사 / 관계언: 조사 / 독립언: 감탄사 / 용언; 동사, 형용사\n",
        "### 이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합으로 구성됩니다. 앞으로 용언이라고 언급하는 부분은 전부 동사와 형용사를 포함하여 언급하는 개념입니다.\n",
        "## 활용 (conjugation)\n",
        "#### 활용이란 용언의 어간(stem)이 어미(ending)를 가지는 일을 말합니다. 활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉩니다.\n",
        "### 어간(stem) : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).\n",
        "### 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행"
      ],
      "metadata": {
        "id": "0MfHWaqHO33N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "print('어간 추출 전 :', tokenized_sentence)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaUK9um_O39R",
        "outputId": "a8c366d4-5707-42ac-8607-2ad9b53a0ff9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
            "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "규칙 기반의 접근을 하고 있으므로 어간 추출 후의 결과에는 사전에 없는 단어들도 포함되어 있습니다. 가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다\n",
        "ALIZE → AL\n",
        "ANCE → 제거\n",
        "ICAL → IC\n",
        "'''\n",
        "\n",
        "words = ['formalize', 'allowance', 'electricical']\n",
        "\n",
        "print('어간 추출 전 :',words)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FQxcsqZPeaL",
        "outputId": "c6b53384-035a-4b8d-8b37-0af8b6eb59cb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['formalize', 'allowance', 'electricical']\n",
            "어간 추출 후 : ['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에서는 포터 알고리즘 외에도 랭커스터 스태머(Lancaster Stemmer) 알고리즘을 지원합니다.\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print('어간 추출 전 :', words)\n",
        "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
        "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQPHj7t_O4Bz",
        "outputId": "0b41ea3a-db42-4f04-fdf4-67fa40e2ee5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
            "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
            "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 불필요한 단어의 제거 - 불용어 제거\n",
        "## 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다."
      ],
      "metadata": {
        "id": "hYPhI8kOLyVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaSZzStORAf3",
        "outputId": "53ee4299-886d-4b6f-d420-4e11de7988a7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "2pBYliP4QrKM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words_list = stopwords.words('english')\n",
        "print('불용어 개수 :', len(stop_words_list))\n",
        "print('불용어 10개 출력 :',stop_words_list[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7ZCbahXRDBH",
        "outputId": "7e79cb8d-a672-4b0d-e9fe-259530a90805"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 : 179\n",
            "불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for word in word_tokens:\n",
        "    if word not in stop_words:\n",
        "        result.append(word)\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens)\n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaPGZelBRD8L",
        "outputId": "0f814266-c703-441f-960b-1e613dab722e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어에서 불용어 제거하기\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
        "\n",
        "stop_words = set(stop_words.split(' '))\n",
        "word_tokens = okt.morphs(example)\n",
        "\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print('불용어 제거 전 :',word_tokens)\n",
        "print('불용어 제거 후 :',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Oe32jI6RD-V",
        "outputId": "ed457ea0-5003-4e21-bafd-876a6877e67a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']\n",
            "불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 정규 표현식"
      ],
      "metadata": {
        "id": "8KQYR5QXRuWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규 표현식의 여러 예제"
      ],
      "metadata": {
        "id": "xpxeLVDIcuee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf1QKq9tLtFE",
        "outputId": "0712bb83-bdcc-40e4-aa7a-b099507a1488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ]
        }
      ],
      "source": [
        "# 3. 불필요한 단어의 제거 - 길이가 짧은 단어 제거\n",
        "# 4. 정규표현식\n",
        "\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "\n",
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .은 한 개의 임의의 문자를 나타냅니다.\n",
        "import re\n",
        "\n",
        "r = re.compile(\"a.c\")\n",
        "\n",
        "print(r.search('abc'))\n",
        "\n",
        "print(r.search('kkk'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Fspex6R-iN",
        "outputId": "78b8df4e-7c6f-4c53-993e-7e0b4e33407d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ?는 ?앞의 문자가 존재할 수도 있고 존재하지 않을 수도 있는 경우를 나타냅니다.\n",
        "# 예를 들어서 정규 표현식이 ab?c라고 합시다. 이 경우 이 정규 표현식에서의 b는 있다고 취급할 수도 있고, 없다고 취급할 수도 있습니다. 즉, abc와 ac 모두 매치할 수 있습니다.\n",
        "r = re.compile(\"ab?c\")\n",
        "\n",
        "print(r.search('abc'))\n",
        "\n",
        "print(r.search('abbc'))\n",
        "\n",
        "print(r.search('ac'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnyEjKfUSwqI",
        "outputId": "892c0709-7140-4461-dde4-8db93f459937"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "None\n",
            "<re.Match object; span=(0, 2), match='ac'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *은 바로 앞의 문자가 0개 이상일 경우를 나타냅니다.\n",
        "# 앞의 문자는 존재하지 않을 수도 있으며, 또는 여러 개일 수도 있습니다.\n",
        "r = re.compile(\"ab*c\")\n",
        "\n",
        "print(r.search(\"a\"))\n",
        "\n",
        "print(r.search(\"ac\"))\n",
        "\n",
        "print(r.search(\"abc\"))\n",
        "\n",
        "print(r.search(\"abbbbc\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGwiytSkS2lC",
        "outputId": "3f44ebdf-af9d-4ce8-d417-d406e6fddbc1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 2), match='ac'>\n",
            "<re.Match object; span=(0, 6), match='abbbbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# +은 바로 앞의 문자가 1개 이상일 경우를 나타냅니다.\n",
        "r = re.compile(\"ab+c\")\n",
        "\n",
        "print(r.search(\"a\"))\n",
        "\n",
        "print(r.search(\"ac\"))\n",
        "\n",
        "print(r.search(\"abc\"))\n",
        "\n",
        "print(r.search(\"abbbbc\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_C4rBRZS2nM",
        "outputId": "599480fe-46f4-4007-9349-782a1cc421f3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 3), match='abc'>\n",
            "<re.Match object; span=(0, 6), match='abbbbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"^ab\")\n",
        "\n",
        "print(r.search(\"bbc\"))\n",
        "\n",
        "print(r.search(\"zab\"))\n",
        "\n",
        "\n",
        "print(r.search(\"abz\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4hSO7cNS2pA",
        "outputId": "0d2c5a63-16dc-41db-a64b-f6f44e775a07"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 2), match='ab'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {숫자} 기호: 문자에 해당 기호를 붙이면, 해당 문자를 숫자만큼 반복한 것을 나타냅니다.\n",
        "# 예를 들어서 정규 표현식이 ab{2}c라면 a와 c 사이에 b가 존재하면서 b가 2개인 문자열에 대해서 매치합니다.\n",
        "\n",
        "r = re.compile(\"ab{2}c\")\n",
        "\n",
        "print(r.search(\"ac\"))\n",
        "\n",
        "print(r.search(\"abc\"))\n",
        "\n",
        "print(r.search(\"abbbbbc\"))\n",
        "\n",
        "print(r.search(\"abbc\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmK1eVvbS2rO",
        "outputId": "0156fbc9-2db0-4897-8aa8-43caeede5337"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 4), match='abbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {숫자1, 숫자2} 기호: 문자에 해당 기호를 붙이면, 해당 문자를 숫자1 이상 숫자2 이하만큼 반복합니다.\n",
        "# 예를 들어서 정규 표현식이 ab{2,8}c라면 a와 c 사이에 b가 존재하면서 b는 2개 이상 8개 이하인 문자열에 대해서 매치합니다.\n",
        "\n",
        "r = re.compile(\"ab{2,8}c\")\n",
        "\n",
        "print(r.search(\"ac\"))\n",
        "\n",
        "print(r.search(\"abc\"))\n",
        "\n",
        "print(r.search(\"abbbbbbbbbc\"))\n",
        "\n",
        "print(r.search(\"abbc\"))\n",
        "\n",
        "print(r.search(\"abbbbbbbbc\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHpPVQg5S2tN",
        "outputId": "64075b85-ab44-4e1d-ce20-d365c026bfea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 4), match='abbc'>\n",
            "<re.Match object; span=(0, 10), match='abbbbbbbbc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {숫자,} 기호: 문자에 해당 기호를 붙이면 해당 문자를 숫자 이상 만큼 반복합니다.\n",
        "# 예를 들어서 정규 표현식이 a{2,}bc라면 뒤에 bc가 붙으면서 a의 개수가 2개 이상인 경우인 문자열과 매치합니다.\n",
        "# 또한 만약 {0,}을 쓴다면 *와 동일한 의미가 되며, {1,}을 쓴다면 +와 동일한 의미가 됩니다.\n",
        "\n",
        "r = re.compile(\"a{2,}bc\")\n",
        "\n",
        "print(r.search(\"bc\"))\n",
        "\n",
        "print(r.search(\"aa\"))\n",
        "\n",
        "print(r.search(\"aabc\"))\n",
        "\n",
        "print(r.search(\"aaaaaaaabc\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0oQ4YTTS2vj",
        "outputId": "53195379-413e-44e9-b906-e0f0bcbc3767"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 4), match='aabc'>\n",
            "<re.Match object; span=(0, 10), match='aaaaaaaabc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [ ] 기호: [ ]안에 문자들을 넣으면 그 문자들 중 한 개의 문자와 매치라는 의미를 가집니다.\n",
        "# 예를 들어서 정규 표현식이 [abc]라면, a 또는 b또는 c가 들어가있는 문자열과 매치됩니다. 범위를 지정하는 것도 가능합니다.\n",
        "# [a-zA-Z]는 알파벳 전부를 의미하며, [0-9]는 숫자 전부를 의미합니다.\n",
        "\n",
        "r = re.compile(\"[abc]\") # [abc]는 [a-c]와 같다.\n",
        "\n",
        "print(r.search('zzz'))\n",
        "\n",
        "print(r.search('a'))\n",
        "\n",
        "print(r.search('aaaaaa'))\n",
        "\n",
        "print(r.search('baac'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agwNIh75Z91F",
        "outputId": "8cfc585e-26ae-4083-c46e-4317d03e470f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(0, 1), match='a'>\n",
            "<re.Match object; span=(0, 1), match='a'>\n",
            "<re.Match object; span=(0, 1), match='b'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"[a-z]\")\n",
        "\n",
        "print(r.search(\"AAA\"))\n",
        "\n",
        "print(r.search(\"111\"))\n",
        "\n",
        "print(r.search('aBC'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jlTLG91abAe",
        "outputId": "09885e4a-b6ed-4d25-a99f-eefb200efe42"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 1), match='a'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [^문자] 기호: [^문자]는 ^기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치하는 역할을 합니다.\n",
        "# 예를 들어서 [^abc]라는 정규 표현식이 있다면, a 또는 b 또는 c가 들어간 문자열을 제외한 모든 문자열을 매치합니다.\n",
        "\n",
        "r = re.compile(\"[^abc]\")\n",
        "\n",
        "print(r.search(\"a\"))\n",
        "\n",
        "print(r.search(\"ab\"))\n",
        "\n",
        "print(r.search(\"b\"))\n",
        "\n",
        "print(r.search(\"d\"))\n",
        "\n",
        "print(r.search(\"1\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph3xzJGbaj0K",
        "outputId": "5fa0299b-3af3-4748-a993-46cc111c2f14"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n",
            "None\n",
            "<re.Match object; span=(0, 1), match='d'>\n",
            "<re.Match object; span=(0, 1), match='1'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.match() 와 re.search()의 차이\n",
        "# search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규 표현식과 매치하는지를 확인합니다.\n",
        "# 문자열 중간에 찾을 패턴이 있더라도 match 함수는 문자열의 시작에서 패턴이 일치하지 않으면 찾지 않습니다.\n",
        "\n",
        "r = re.compile(\"ab.\")\n",
        "\n",
        "print(r.match(\"kkkabc\"))\n",
        "\n",
        "print(r.search(\"kkkabc\"))\n",
        "\n",
        "print(r.match(\"abckkk\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fy9k9i7bLY-",
        "outputId": "e9cbe65a-9167-460f-ea0a-286b7a55c958"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "<re.Match object; span=(3, 6), match='abc'>\n",
            "<re.Match object; span=(0, 3), match='abc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.split\n",
        "\n",
        "# 공백 기준 분리\n",
        "text = \"사과 딸기 수박 메론 바나나\"\n",
        "re.split(\" \", text)\n",
        "\n",
        "# 줄바꿈 기준 분리\n",
        "text = \"\"\"사과\n",
        "딸기\n",
        "수박\n",
        "메론\n",
        "바나나\"\"\"\n",
        "\n",
        "re.split(\"\\n\", text)\n",
        "\n",
        "# '+'를 기준으로 분리\n",
        "text = \"사과+딸기+수박+메론+바나나\"\n",
        "\n",
        "re.split(\"\\+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaNf2oM_bciA",
        "outputId": "d6211a45-72b0-4512-81a7-8e879d59de42"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사과', '딸기', '수박', '메론', '바나나']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.findall()\n",
        "\n",
        "text = \"\"\"이름 : 김철수\n",
        "전화번호 : 010 - 1234 - 1234\n",
        "나이 : 30\n",
        "성별 : 남\"\"\"\n",
        "\n",
        "re.findall(\"\\d+\", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e3_ueu0b0En",
        "outputId": "5ae1d206-f5fd-491b-e1c0-aa6d963978fc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['010', '1234', '1234', '30']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"\\d+\", \"문자열입니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj4g3tawb2XY",
        "outputId": "13d3ddbc-067f-4983-f8fb-f0cc9df26337"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re.sub()\n",
        "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
        "\n",
        "preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuynDFMbcP4c",
        "outputId": "1bb9115d-4072-4cb6-d085-08c5aaf682c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"100 John    PROF\n",
        "101 James   STUD\n",
        "102 Mac   STUD\"\"\"\n",
        "\n",
        "re.split('\\s+', text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqqNuSiocS_q",
        "outputId": "52b7476f-f820-4173-8ce7-9e4909f979f1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('\\d+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDp5N9gcflz",
        "outputId": "664abaac-54d0-4a31-8805-b51e52e296be"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['100', '101', '102']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('[A-Z]',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4GcRo13ch0O",
        "outputId": "0ed09292-ee64-4ddf-ba98-2a6138bfc490"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('[A-Z]{4}',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81SR9raZcjg7",
        "outputId": "ee5c16f9-8d89-4f41-8e10-f127a4979e2a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PROF', 'STUD', 'STUD']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall('[A-Z][a-z]+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mku15Qicnga",
        "outputId": "1ce9097e-fff5-4eb8-b74d-fb776deb5729"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['John', 'James', 'Mac']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규 표현식을 이용한 토큰화"
      ],
      "metadata": {
        "id": "Xls7nRVocsDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
        "\n",
        "tokenizer1 = RegexpTokenizer(\"[\\w]+\") # 문자 또는 숫자가 1개 이상\n",
        "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True) # 공백을 기준으로\n",
        "\n",
        "print(tokenizer1.tokenize(text))\n",
        "print(tokenizer2.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbflnTQMcrbG",
        "outputId": "d204e1f9-1af3-4314-bc8f-95fe15cd83aa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
            "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    }
  ]
}